{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1ab0530",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b271934",
   "metadata": {},
   "source": [
    "## 1. Derivation of the Logistic Regression Objective Function Using MLE (see reference[3])\n",
    "\n",
    "We consider a binary classification problem with training data $ \\{(\\mathbf{x}^{(i)}, y^{(i)})\\}_{i=1}^n $, where $ \\mathbf{x}^{(i)} \\in \\mathbb{R}^d $ is the feature vector and $ y^{(i)} \\in \\{0, 1\\} $ is the label for the $ i $-th data point.\n",
    "\n",
    "We assume the following:\n",
    "- The data points are independent and identically distributed (I will use i.d.d in the future)\n",
    "- The conditional probability of the label is as follows:\n",
    "\n",
    "$$\n",
    "P(y = 1 \\mid \\mathbf{x}; \\boldsymbol{\\theta}) = \\sigma(\\mathbf{x}^\\top \\boldsymbol{\\theta}) = \\frac{1}{1 + e^{-\\mathbf{x}^\\top \\boldsymbol{\\theta}}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(y = 0 \\mid \\mathbf{x}; \\boldsymbol{\\theta}) = 1 - \\sigma(\\mathbf{x}^\\top \\boldsymbol{\\theta})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Combind our Conditional Probability Expressions\n",
    "\n",
    "$$\n",
    "P(y \\mid \\mathbf{x}; \\boldsymbol{\\theta}) =\n",
    "\\sigma(\\mathbf{x}^\\top \\boldsymbol{\\theta})^y \\cdot (1 - \\sigma(\\mathbf{x}^\\top \\boldsymbol{\\theta}))^{1 - y}\n",
    "$$\n",
    "\n",
    "This works because:\n",
    "- If $ y = 1 $, then the expression becomes $ \\sigma(\\mathbf{x}^\\top \\boldsymbol{\\theta}) $\n",
    "- If $ y = 0 $, then it becomes $ 1 - \\sigma(\\mathbf{x}^\\top \\boldsymbol{\\theta}) $\n",
    "\n",
    "---\n",
    "\n",
    "### Likelihood of the Dataset\n",
    "\n",
    "If like in our original assumtion the data is i.i.d., the likelihood of the full dataset is the product of the individual probabilities:\n",
    "\n",
    "$$\n",
    "L(\\boldsymbol{\\theta}) = \\prod_{i=1}^n P(y^{(i)} \\mid \\mathbf{x}^{(i)}; \\boldsymbol{\\theta}) =\n",
    "\\prod_{i=1}^n \\left[\n",
    "\\sigma(\\mathbf{x}^{(i)\\top} \\boldsymbol{\\theta})^{y^{(i)}} \\cdot (1 - \\sigma(\\mathbf{x}^{(i)\\top} \\boldsymbol{\\theta}))^{1 - y^{(i)}}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Log Likelihood \n",
    "\n",
    "To simplify the product we take the logarithm of the likelihood:\n",
    "\n",
    "$$\n",
    "\\ell(\\boldsymbol{\\theta}) = \\log L(\\boldsymbol{\\theta}) = \\log \\prod_{i=1}^n a_i = \\sum_{i=1}^n \\log a_i\n",
    "$$\n",
    "\n",
    "We now substitue $a_i$ with our expression from earlier:\n",
    "\n",
    "$$\n",
    "a_i = \\sigma(\\mathbf{x}^{(i)\\top} \\boldsymbol{\\theta})^{y^{(i)}} \\cdot (1 - \\sigma(\\mathbf{x}^{(i)\\top} \\boldsymbol{\\theta}))^{1 - y^{(i)}}\n",
    "$$\n",
    "\n",
    "Using $ \\log(ab) = \\log a + \\log b $:\n",
    "\n",
    "$$\n",
    "\\log a_i = y^{(i)} \\log \\sigma(\\mathbf{x}^{(i)\\top} \\boldsymbol{\\theta}) +\n",
    "(1 - y^{(i)}) \\log (1 - \\sigma(\\mathbf{x}^{(i)\\top} \\boldsymbol{\\theta}))\n",
    "$$\n",
    "\n",
    "Which gives us:\n",
    "\n",
    "$$\n",
    "\\ell(\\boldsymbol{\\theta}) = \\sum_{i=1}^n \\left[\n",
    "y^{(i)} \\log \\sigma(\\mathbf{x}^{(i)\\top} \\boldsymbol{\\theta}) +\n",
    "(1 - y^{(i)}) \\log (1 - \\sigma(\\mathbf{x}^{(i)\\top} \\boldsymbol{\\theta}))\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Objective Function\n",
    "\n",
    "Our goal is to maximize the log likelihood, However, as instructed in class we should we minimize the negative log likelihood:\n",
    "\n",
    "$$\n",
    "W(\\boldsymbol{\\theta}) = -\\ell(\\boldsymbol{\\theta}) =\n",
    "- \\sum_{i=1}^n \\left[\n",
    "y^{(i)} \\log \\sigma(\\mathbf{x}^{(i)\\top} \\boldsymbol{\\theta}) +\n",
    "(1 - y^{(i)}) \\log (1 - \\sigma(\\mathbf{x}^{(i)\\top} \\boldsymbol{\\theta}))\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "## 2. MAP vs MLE (see reference 4)\n",
    "\n",
    "### Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "- MLE chooses the parameters $ \\boldsymbol{\\theta} $ that make the observed data most likely\n",
    "- MLE treats the parameters as fixed but unknown quantities.\n",
    "- MLE does not include any prior beliefs about $ \\boldsymbol{\\theta}, it only uses the observed data.\n",
    "\n",
    "MLE can be defined as:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\theta}}_{\\text{MLE}} = \\arg\\max_{\\boldsymbol{\\theta}} \\; P(\\mathcal{D} \\mid \\boldsymbol{\\theta})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\mathcal{D} $ is the dataset (with inputs $ \\mathbf{X} $ and labels $ \\mathbf{y} $),\n",
    "- $ P(\\mathcal{D} \\mid \\boldsymbol{\\theta}) $ is the likelihood of the data given the parameters.\n",
    "\n",
    "### Maximum A Posteriori Estimation (MAP)\n",
    "\n",
    "MAP extends MLE by including a prior belief about the parameters. It uses Bayes’ theorem to compute a posterior distribution over parameters:\n",
    "\n",
    "$$\n",
    "P(\\boldsymbol{\\theta} \\mid \\mathcal{D}) = \\frac{P(\\mathcal{D} \\mid \\boldsymbol{\\theta}) \\cdot P(\\boldsymbol{\\theta})}{P(\\mathcal{D})}\n",
    "$$\n",
    "\n",
    "MAP then chooses the parameters that maximize this:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\theta}}_{\\text{MAP}} = \\arg\\max_{\\boldsymbol{\\theta}} \\; P(\\boldsymbol{\\theta} \\mid \\mathcal{D}) = \\arg\\max_{\\boldsymbol{\\theta}} \\; P(\\mathcal{D} \\mid \\boldsymbol{\\theta}) \\cdot P(\\boldsymbol{\\theta})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ P(\\boldsymbol{\\theta}) $ is the prior distribution over parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8b6c56",
   "metadata": {},
   "source": [
    "## 2. Machine Learning Problem and Model Justification (see reference 7)\n",
    "\n",
    "### Problem Definition\n",
    "\n",
    "I wish to use logistic regression to classify particle collision events as either signal (Higgs boson production) or background (standard processes).\n",
    "\n",
    "We can formulate this as a binary classification task by making:\n",
    "- `1` = signal event\n",
    "- `0` = background event\n",
    "\n",
    "### Why Should We Use Logistic Regression?\n",
    "\n",
    "- Logistic regression is built for binary classification problems like signal vs. background\n",
    "- Logistic regression is gives probabilities (For example, 80% chance of signal), which helps physicists decide if an event is likely Higgs boson production.\n",
    "- Collision data often has features that can be separated linearly\n",
    "- Logistic regression is more resistant to noisy data from particle collisions which can help avoid overfitting\n",
    "\n",
    "### Comparison to Linear Support Vector Machine (Linear SVM)\n",
    "\n",
    "#### Similarity:\n",
    "- Both create a straight line to separate signal from background events\n",
    "- Both work well with large datasets and can handle many features\n",
    "- Both use regularization to avoid overfitting\n",
    "\n",
    "#### Difference:\n",
    "- Logistic regression gives probabilities whereas Linear SVM gives a yes/no classification\n",
    "- Logistic regression optimizes for accurate probabilities. Linear SVM focuses on maximizing the gap between signal and background\n",
    "- Logistic regression is better with noisy data whereas Linear SVM can be affected more by outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84fce4e",
   "metadata": {},
   "source": [
    "## 3. Dataset and Equation Correspondence\n",
    "\n",
    "In the derivation of logistic regression, each training instance was defined as a pair $ (\\mathbf{x}^{(i)}, y^{(i)}) $, where:\n",
    "\n",
    "- $ \\mathbf{x}^{(i)} \\in \\mathbb{R}^d $: a feature vector  \n",
    "- $ y^{(i)} \\in \\{0, 1\\} $: a binary label  \n",
    "- $ \\boldsymbol{\\theta} \\in \\mathbb{R}^d $: model parameters\n",
    "\n",
    "This maps directly to the HIGGS dataset as follows:\n",
    "\n",
    "- Each row in the dataset is a collision event, so $ i = 1, \\dots, n $ with $ n \\approx 11,000,000 $\n",
    "- The feature vector $ \\mathbf{x}^{(i)} $ consists of 28 features per event including momentum, transverse energy, etc\n",
    "- The label $ y^{(i)} $ is a binary indicator: `1` for signal, `0` for background\n",
    "\n",
    "### Modeling Assumptions\n",
    "\n",
    "- We assume each event is treated as conditionally independent of the others, which is reasonable given the simulation based nature of the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22714aa5",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913b1729",
   "metadata": {},
   "source": [
    "## 1. Dataset Selection\n",
    "\n",
    "For this task, I selected the **HIGGS dataset** from the UCI Machine Learning Repository.\n",
    "\n",
    "- **Dataset link**: [https://archive.ics.uci.edu/dataset/280/higgs](https://archive.ics.uci.edu/dataset/280/higgs)\n",
    "- **Instances**: 11,000,000\n",
    "- **Features**: 28 per event\n",
    "- **Target**: Binary classification\n",
    "  - `1` = signal event \n",
    "  - `0` = background event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbdea1f",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Cornell University. (n.d.). *Logistic Regression*. CS 4780: Machine Learning.  \n",
    "   Retrieved from https://www.cs.cornell.edu/courses/cs4780/2015fa/web/lecturenotes/lecturenote06.html\n",
    "\n",
    "2. Murphy, K. P. (2022). *Probabilistic Machine Learning: An Introduction*. MIT Press.  \n",
    "   Retrieved from https://www.cs.ubc.ca/~murphyk/PMLbook/book1.html\n",
    "\n",
    "3. NucleusBox. (n.d.). *Cost Function in Logistic Regression – Understanding the Theory Behind the Loss*.  \n",
    "   Retrieved from https://www.nucleusbox.com/cost-function-in-logistic-regression/\n",
    "\n",
    "4. Schmidt, M. (2017). *MLE and MAP Estimation* [Lecture slides]. CPSC 340, University of British Columbia.  \n",
    "   Retrieved from https://www.cs.ubc.ca/~schmidtm/Courses/340-F17/L25.pdf\n",
    "\n",
    "5. University of Pennsylvania. (n.d.). *Logistic Regression - CIS 520 Machine Learning*.  \n",
    "   Retrieved from https://alliance.seas.upenn.edu/~cis520/wiki/index.php?n=Lectures.Logistic\n",
    "\n",
    "6. Wu, S. (n.d.). *Lecture 5: Logistic Regression* [PDF]. CSCI 5525 - Machine Learning, University of Minnesota.  \n",
    "   Retrieved from https://zstevenwu.com/courses/s20/csci5525/resources/slides/lecture05.pdf\n",
    "   \n",
    "7. Cortes, C., & Vapnik, V. (1995). *Support-vector networks*. Machine Learning, 20(3), 273–297.  \n",
    "   Retrieved from https://doi.org/10.1007/BF00994018\n",
    "\n",
    "8. QuickRef. (n.d.). *LaTeX Math Symbols Cheat Sheet*.  \n",
    "   Retrieved from https://quickref.me/latex.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml-pytorch)",
   "language": "python",
   "name": "ml-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
