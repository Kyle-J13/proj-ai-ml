{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07824222",
   "metadata": {},
   "source": [
    "## Scaled dot-product Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "359fe770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Scaled dot-procuct attention\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "\n",
    "# Softmax\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "# Scaled dot product attention (reference [1])\n",
    "# Q, K, V are numpy arrays with shape (batch_size, seq_len, d_k)\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    d_k = Q.shape[-1]\n",
    "    scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)\n",
    "    weights = softmax(scores)\n",
    "    output = np.matmul(weights, V)\n",
    "    return output, weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95314b32",
   "metadata": {},
   "source": [
    "## Integrating our Scaled dot-product Attention Mechanism into the encoder of a Seq2Seq Encoder/Decoder Style model based on Bahdanau attention (see refernce[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6267824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class EncoderRNNWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Linear layers to produce Q, K, V\n",
    "        self.to_Q = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.to_K = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.to_V = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src: (batch_size, seq_len)\n",
    "        embedded = self.embedding(src)   # (batch_size, seq_len, embedding_dim)\n",
    "        embedded = self.dropout(embedded)   # apply dropout\n",
    "        outputs, hidden = self.rnn(embedded)  # (batch, seq_len, hidden_dim)\n",
    "\n",
    "        # Q, K, V from encoder hidden states\n",
    "        Q = self.to_Q(outputs).detach().cpu().numpy()\n",
    "        K = self.to_K(outputs).detach().cpu().numpy()\n",
    "        V = self.to_V(outputs).detach().cpu().numpy()\n",
    "\n",
    "        # Use our attention from earlier\n",
    "        with torch.no_grad():\n",
    "            context_np, _ = scaled_dot_product_attention(Q, K, V)\n",
    "        context = torch.tensor(context_np, dtype=torch.float32).to(outputs.device)\n",
    "\n",
    "        return context, hidden\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, tgt, context, hidden):\n",
    "        # encoders final state (1, batch, hidden_dim)\n",
    "        init_hidden = hidden\n",
    "\n",
    "        # embed all target tokens\n",
    "        embedded = self.embedding(tgt)    # (batch, tgt_len, embedding_dim)\n",
    "        embedded = self.dropout(embedded)   # apply dropout\n",
    "\n",
    "        # decode using encoder state as init\n",
    "        outputs, last_hidden = self.rnn(embedded, init_hidden)\n",
    "\n",
    "        # map to vocabulary logits\n",
    "        logits = self.fc_out(outputs)  # (batch, tgt_len, vocab_size)\n",
    "\n",
    "        return logits, last_hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "32cb2a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop for encoder-decoder model\n",
    "def train(model_enc, model_dec, dataloader, optimizer, criterion, device):\n",
    "    model_enc.train()\n",
    "    model_dec.train()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for src, tgt in dataloader:\n",
    "        # Move input and target to device\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass through encoder\n",
    "        context, enc_hidden = model_enc(src)\n",
    "\n",
    "        # Forward pass through decoder\n",
    "        logits, _ = model_dec(tgt[:, :-1], context, enc_hidden)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits.reshape(-1, logits.shape[-1]), tgt[:, 1:].reshape(-1))\n",
    "\n",
    "        # Backprop and update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96a55dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example pair: ('Take a seat.', 'Prends place !')\n"
     ]
    }
   ],
   "source": [
    "# Load data (see reference[3])\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "\n",
    "# Load CSV \n",
    "df = pd.read_csv(\"eng_-french.csv\")\n",
    "df = df.dropna().sample(n=50000, random_state=42)\n",
    "\n",
    "# Rename columns for ease of use\n",
    "df.columns = [\"english\", \"french\"]\n",
    "\n",
    "# Create sentence pairs\n",
    "sentence_pairs = list(zip(df[\"english\"], df[\"french\"]))\n",
    "\n",
    "# Preview\n",
    "print(\"Example pair:\", sentence_pairs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cae41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from collections import defaultdict\n",
    "\n",
    "# Tokenizer\n",
    "def tokenize(sentence):\n",
    "    return sentence.lower().strip().split()\n",
    "\n",
    "# Build vocab (see reference[4])\n",
    "def build_vocab(sentences):\n",
    "    vocab = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "    index = 4\n",
    "    for sent in sentences:\n",
    "        for token in tokenize(sent):\n",
    "            if token not in vocab:\n",
    "                vocab[token] = index\n",
    "                index += 1\n",
    "    return vocab\n",
    "\n",
    "# Encode a sentence as token IDs\n",
    "def encode(sentence, vocab, max_len=100):\n",
    "    tokens = [\"<SOS>\"] + tokenize(sentence) + [\"<EOS>\"]\n",
    "    ids = [vocab.get(t, vocab[\"<UNK>\"]) for t in tokens]\n",
    "    ids = ids[:max_len] + [vocab[\"<PAD>\"]] * (max_len - len(ids))\n",
    "    return ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c6a27832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split source and target sentences\n",
    "src_sentences = [src for src, _ in sentence_pairs]\n",
    "tgt_sentences = [tgt for _, tgt in sentence_pairs]\n",
    "\n",
    "# Build vocabularies\n",
    "source_vocab = build_vocab(src_sentences)\n",
    "target_vocab = build_vocab(tgt_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3cc8a7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Dataset for loading and encoding translation pairs\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, pairs, src_vocab, tgt_vocab, max_len=100):\n",
    "        # Store sentence pairs and vocabularies\n",
    "        self.pairs = pairs\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return total number of examples\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve source and target sentences\n",
    "        src, tgt = self.pairs[idx]\n",
    "        # Encode to fixed‚Äêlength token ID lists\n",
    "        src_ids = encode(src, self.src_vocab, self.max_len)\n",
    "        tgt_ids = encode(tgt, self.tgt_vocab, self.max_len)\n",
    "        # Convert lists to long tensors\n",
    "        return (\n",
    "            torch.tensor(src_ids, dtype=torch.long),\n",
    "            torch.tensor(tgt_ids, dtype=torch.long),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c55e235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create DataLoader for batching and shuffling\n",
    "train_dataset = TranslationDataset(sentence_pairs, source_vocab, target_vocab)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,  \n",
    "    shuffle=True   \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8acfc889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch.nn as nn\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize encoder and decoder models on device\n",
    "encoder = EncoderRNNWithAttention(len(source_vocab), 128, 256).to(device)\n",
    "decoder = DecoderRNN(len(target_vocab), 128, 256).to(device)\n",
    "\n",
    "# Loss function (ignore padding) and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=target_vocab[\"<PAD>\"])\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(encoder.parameters()) + list(decoder.parameters()),\n",
    "    lr=0.001  # learning rate\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "848d29c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 4.7643\n",
      "Epoch 2: Loss = 3.5288\n",
      "Epoch 3: Loss = 2.9924\n",
      "Epoch 4: Loss = 2.6298\n",
      "Epoch 5: Loss = 2.3622\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    loss = train(encoder, decoder, train_loader, optimizer, criterion, device)\n",
    "    # Print loss \n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f41f7cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.0397\n"
     ]
    }
   ],
   "source": [
    "# Imports for BLEU evaluation\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def greedy_translate(src_sentence):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    # Prepare source tensor\n",
    "    src_ids = encode(src_sentence, source_vocab)\n",
    "    src_tensor = torch.tensor(src_ids).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get context and initial hidden state from encoder\n",
    "        context, enc_hidden = encoder(src_tensor)\n",
    "        hidden = enc_hidden\n",
    "        tgt_ids = [target_vocab[\"<SOS>\"]]\n",
    "\n",
    "        # Decode \n",
    "        for _ in range(40):\n",
    "            tgt_tensor = torch.tensor(tgt_ids).unsqueeze(0).to(device)\n",
    "            # Unpack logits and hidden state\n",
    "            logits, hidden = decoder(tgt_tensor, context, hidden)\n",
    "            next_token = logits[0, -1].argmax().item()\n",
    "            if next_token == target_vocab[\"<EOS>\"]:\n",
    "                break\n",
    "            tgt_ids.append(next_token)\n",
    "\n",
    "    return tgt_ids[1:]\n",
    "\n",
    "# Reverse vocab for decoding\n",
    "inv_tgt_vocab = {i: w for w, i in target_vocab.items()}\n",
    "\n",
    "# BLEU eval over 1000 examples\n",
    "references = []\n",
    "hypotheses = []\n",
    "\n",
    "for src, tgt in sentence_pairs[:1000]:\n",
    "    pred_ids = greedy_translate(src)\n",
    "    ref_ids = encode(tgt, target_vocab)\n",
    "\n",
    "    # Remove PAD, SOS, EOS tokens\n",
    "    clean_ref = [i for i in ref_ids if i not in [0, 1, 2]]\n",
    "    clean_pred = [i for i in pred_ids if i not in [0, 1, 2]]\n",
    "\n",
    "    references.append([clean_ref])\n",
    "    hypotheses.append(clean_pred)\n",
    "\n",
    "bleu_score = corpus_bleu(references, hypotheses)\n",
    "print(f\"BLEU Score: {bleu_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9a3222",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "As you can see it did not have that great of a blue score. I want to attribute this to the fact that I do not have the attention mechanism in the decoder.\n",
    "Maybe there is somthing else that I am missing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a35fa7",
   "metadata": {},
   "source": [
    "## Simplified Transformer Implementation (see reference[5])\n",
    "This was difficult task so I heavily refernced [5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06033a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# Positional encoding using sinusoidal functions\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1).float()  # (max_len, 1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)  # sine on even dimensions\n",
    "        pe[:, 1::2] = torch.cos(pos * div)  # cosine on odd dimensions\n",
    "        self.register_buffer('pe', pe)  # store encoding as buffer \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:seq_len]  # add positional encoding to input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30495b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module): # https://docs.pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html\n",
    "    def __init__(self, d_model=64, n_heads=2):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0  # make sure heads divide evenly\n",
    "        self.d_k = d_model // n_heads  # head dimension\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.W_Q = nn.Linear(d_model, d_model)  # linear projection for queries\n",
    "        self.W_K = nn.Linear(d_model, d_model)  # linear projection for keys\n",
    "        self.W_V = nn.Linear(d_model, d_model)  # linear projection for values\n",
    "        self.W_O = nn.Linear(d_model, d_model)  # output projection\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        batch = Q.size(0)\n",
    "\n",
    "        # linear projection and head split\n",
    "        def split(x, W):\n",
    "            x = W(x)  # (batch, seq, d_model)\n",
    "            x = x.view(batch, -1, self.n_heads, self.d_k).transpose(1, 2)  # (batch, heads, seq, d_k)\n",
    "            return x\n",
    "\n",
    "        Qh, Kh, Vh = split(Q, self.W_Q), split(K, self.W_K), split(V, self.W_V)\n",
    "\n",
    "        # compute scaled dot-product attention\n",
    "        scores = (Qh @ Kh.transpose(-2, -1)) / math.sqrt(self.d_k)  # (batch, heads, seq, seq)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)  # apply mask\n",
    "        A = torch.softmax(scores, dim=-1)  # attention weights\n",
    "\n",
    "        # combine heads\n",
    "        out = (A @ Vh).transpose(1, 2).contiguous().view(batch, -1, self.n_heads * self.d_k)\n",
    "        return self.W_O(out), A  # final output and attention weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961fb322",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model=64, d_ff=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),  # expand hidden size\n",
    "            nn.ReLU(),  # non-linearity\n",
    "            nn.Linear(d_ff, d_model),  # project back to model dim\n",
    "            nn.Dropout(dropout)  # regularization\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # apply feedforward network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eee735",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model=64, n_heads=2, d_ff=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, n_heads)  # multi-head self attention\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)  # position-wise feedforward\n",
    "        self.norm1 = nn.LayerNorm(d_model)  # layer norm after attention\n",
    "        self.norm2 = nn.LayerNorm(d_model)  # layer norm after FFN\n",
    "        self.dropout = nn.Dropout(dropout)  # dropout for regularization\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # apply self-attention with residual and norm\n",
    "        attn_out, _ = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "        # apply feedforward with residual and norm\n",
    "        ff_out = self.ff(x)\n",
    "        return self.norm2(x + self.dropout(ff_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095aa641",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model=64, n_heads=2, d_ff=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, n_heads)  # masked self-attention\n",
    "        self.cross_attn = MultiHeadAttention(d_model, n_heads)  # encoder-decoder attention\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)  # position-wise feedforward\n",
    "        self.norm1 = nn.LayerNorm(d_model)  # norm after self-attn\n",
    "        self.norm2 = nn.LayerNorm(d_model)  # norm after cross-attn\n",
    "        self.norm3 = nn.LayerNorm(d_model)  # norm after FFN\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask=None, tgt_mask=None):\n",
    "        sa_out, _ = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(sa_out))\n",
    "        # apply encoder-decoder cross-attn with residual and norm\n",
    "        ca_out, _ = self.cross_attn(x, enc_out, enc_out, src_mask)\n",
    "        x = self.norm2(x + self.dropout(ca_out))\n",
    "        # apply feedforward with residual and norm\n",
    "        ff_out = self.ff(x)\n",
    "        return self.norm3(x + self.dropout(ff_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce2782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=64, n_heads=2,\n",
    "                 d_ff=128, num_enc=2, num_dec=2, max_len=100, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.src_tok = nn.Embedding(src_vocab_size, d_model)  # source token embedding\n",
    "        self.tgt_tok = nn.Embedding(tgt_vocab_size, d_model)  # target token embedding\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len)  # sinusoidal position encoding\n",
    "\n",
    "        self.enc_layers = nn.ModuleList([EncoderLayer(d_model,n_heads,d_ff,dropout)\n",
    "                                         for _ in range(num_enc)])  # encoder stack\n",
    "        self.dec_layers = nn.ModuleList([DecoderLayer(d_model,n_heads,d_ff,dropout)\n",
    "                                         for _ in range(num_dec)])  # decoder stack\n",
    "        self.out_proj = nn.Linear(d_model, tgt_vocab_size)  # final projection layer\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        # encode source\n",
    "        enc = self.pos_enc(self.src_tok(src))\n",
    "        for layer in self.enc_layers:\n",
    "            enc = layer(enc, src_mask)\n",
    "        # decode target\n",
    "        dec = self.pos_enc(self.tgt_tok(tgt))\n",
    "        for layer in self.dec_layers:\n",
    "            dec = layer(dec, enc, src_mask, tgt_mask)\n",
    "        # output vocab distribution\n",
    "        return self.out_proj(dec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383b8828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss = 4.9036\n",
      "Epoch 2, Loss = 3.4696\n",
      "Epoch 3, Loss = 2.8391\n",
      "Epoch 4, Loss = 2.4010\n",
      "Epoch 5, Loss = 2.0820\n",
      "Epoch 6, Loss = 1.8397\n",
      "Epoch 7, Loss = 1.6505\n",
      "Epoch 8, Loss = 1.5103\n",
      "Epoch 9, Loss = 1.4020\n",
      "Epoch 10, Loss = 1.3156\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# Dataset definition\n",
    "class SimpleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, pairs, src_vocab, tgt_vocab, max_len=50):\n",
    "        self.pairs = pairs\n",
    "        self.sv = src_vocab\n",
    "        self.tv = tgt_vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        src, tgt = self.pairs[i]\n",
    "        src_ids = encode(src, self.sv, self.max_len)\n",
    "        tgt_ids = encode(tgt, self.tv, self.max_len)\n",
    "        return torch.tensor(src_ids), torch.tensor(tgt_ids)\n",
    "\n",
    "# Collate function\n",
    "def collate(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src = torch.stack(src_batch)\n",
    "    tgt = torch.stack(tgt_batch)\n",
    "    return src, tgt\n",
    "\n",
    "# Create dataloader\n",
    "dataset = SimpleDataset(sentence_pairs, source_vocab, target_vocab)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate)\n",
    "\n",
    "# Create masks\n",
    "def make_src_mask(src):\n",
    "    return (src != source_vocab['<PAD>']).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "def make_tgt_mask(tgt):\n",
    "    seq_len = tgt.size(1)\n",
    "    pad_mask = (tgt != target_vocab['<PAD>']).unsqueeze(1).unsqueeze(3)\n",
    "    subseq = torch.triu(torch.ones((seq_len, seq_len)), diagonal=1).bool()\n",
    "    return pad_mask & ~subseq.to(pad_mask.device)\n",
    "\n",
    "# Initialize model, optimizer, loss\n",
    "model = Transformer(len(source_vocab), len(target_vocab), d_model=64, n_heads=2,\n",
    "                    d_ff=128, num_enc=2, num_dec=2, max_len=50, dropout=0.1).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "crit = nn.CrossEntropyLoss(ignore_index=target_vocab['<PAD>'])\n",
    "\n",
    "# Training loop\n",
    "NUM_EPOCHS = 10\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src, tgt in loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        opt.zero_grad()\n",
    "        src_mask = make_src_mask(src)\n",
    "        tgt_mask = make_tgt_mask(tgt[:,:-1])\n",
    "        out = model(src, tgt[:,:-1], src_mask, tgt_mask)\n",
    "        loss = crit(out.view(-1, out.size(-1)), tgt[:,1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch}, Loss = {total_loss/len(loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784861d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer BLEU Score: 0.1779\n"
     ]
    }
   ],
   "source": [
    "# set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "refs, hyps = [], []\n",
    "\n",
    "# number of evaluation pairs\n",
    "MAX_EVAL_PAIRS = 1000\n",
    "processed = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for src, tgt in loader:\n",
    "        if processed >= MAX_EVAL_PAIRS:\n",
    "            break\n",
    "\n",
    "        # clip batch if it would exceed limit\n",
    "        batch_size = src.size(0)\n",
    "        if processed + batch_size > MAX_EVAL_PAIRS:\n",
    "            batch_size = MAX_EVAL_PAIRS - processed\n",
    "            src = src[:batch_size]\n",
    "            tgt = tgt[:batch_size]\n",
    "\n",
    "        src = src.to(device)\n",
    "        src_mask = make_src_mask(src)\n",
    "\n",
    "        # start sequence with <SOS> token\n",
    "        ys = torch.full((batch_size, 1), target_vocab['<SOS>'], dtype=torch.long).to(device)\n",
    "\n",
    "        # autoregressive decoding\n",
    "        for _ in range(49):\n",
    "            tgt_mask = make_tgt_mask(ys)\n",
    "            out = model(src, ys, src_mask, tgt_mask)\n",
    "            next_tok = out[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "            ys = torch.cat([ys, next_tok], dim=1)\n",
    "\n",
    "        # decode predictions and references\n",
    "        for i in range(batch_size):\n",
    "            pred = ys[i, 1:].cpu().tolist()\n",
    "            ref  = tgt[i, 1:].cpu().tolist()\n",
    "            pred = [t for t in pred if t not in [0, 1, 2]]\n",
    "            ref  = [t for t in ref  if t not in [0, 1, 2]]\n",
    "            refs.append([ref])\n",
    "            hyps.append(pred)\n",
    "\n",
    "        processed += batch_size\n",
    "\n",
    "# BLEU score\n",
    "bleu = corpus_bleu(refs, hyps)\n",
    "print(f\"Transformer BLEU Score: {bleu:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f26993b",
   "metadata": {},
   "source": [
    "### BLEU Score Comparison\n",
    "\n",
    "- RNN with Scaled Dot-Product Attention: BLEU = 0.0397\n",
    "- Simplified Transformer: BLEU = 0.1779\n",
    "\n",
    "### Explanation of Differences in Performance\n",
    "\n",
    "- The transformer handles longer sequences better because of self-attention unlike the RNN which processes sequentially.\n",
    "- Multi-head attention in the transformer allows it to capture multiple types of relationships in the data.\n",
    "- The transformer includes attention in both the encoder and decoder, while the RNN used attention only in the encoder.\n",
    "- Positional encoding in the transformer helps preserve word order without recurrence.\n",
    "\n",
    "### Runtime Differences\n",
    "\n",
    "- The transformer trains faster because it processes sequences in parallel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842675c7",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, ≈Å., & Polosukhin, I. (2017). *Attention Is All You Need*.  \n",
    "   Retrieved from https://arxiv.org/pdf/1706.03762\n",
    "\n",
    "2. PyTorch. (n.d.). *Sequence to Sequence Translation Tutorial*.  \n",
    "   Retrieved from https://docs.pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "\n",
    "3. Devicharith. (2021). *Language Translation - English to French* [Dataset].  \n",
    "   Retrieved from https://www.kaggle.com/datasets/devicharith/language-translation-englishfrench\n",
    "\n",
    "4. Battu, A. (2022). *Understanding How a Seq2Seq Model Works for Machine Translation ‚Äî Comprehensive Explanation for Each Component*.  \n",
    "   Retrieved from https://medium.com/@abhinavbattu88/understanding-how-a-seq2seq-model-works-for-machine-translation-comprehensive-explanation-for-each-d1d872d67e9a\n",
    "\n",
    "5. Bird of Paradise. (2023). *Transformer from Scratch Tutorial* [Notebook].  \n",
    "   Retrieved from https://huggingface.co/datasets/bird-of-paradise/transformer-from-scratch-tutorial/blob/main/Transformer_Implementation_Tutorial.ipynb\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
